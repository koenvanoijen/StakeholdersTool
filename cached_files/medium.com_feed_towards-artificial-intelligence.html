https://medium.com/feed/towards-artificial-intelligence
<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
<channel>
<title><![CDATA[Towards AI - Medium]]></title>
<description><![CDATA[The leading AI community and content platform focused on making AI accessible to all - Medium]]></description>
<link/>https://pub.towardsai.net?source=rss----98111c9905da---4
        <image/>
<url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
<title>Towards AI - Medium</title>
<link/>https://pub.towardsai.net?source=rss----98111c9905da---4
        
        <generator>Medium</generator>
<lastbuilddate>Sat, 17 Jun 2023 09:39:28 GMT</lastbuilddate>
<atom:link href="https://pub.towardsai.net/feed" rel="self" type="application/rss+xml"></atom:link>
<webmaster><![CDATA[yourfriends@medium.com]]></webmaster>
<atom:link href="http://medium.superfeedr.com" rel="hub"></atom:link>
<item>
<title><![CDATA[Managing an AI developer: lessons learnt from SMOL AI — Part 1]]></title>
<link/>https://pub.towardsai.net/managing-an-ai-developer-lessons-learnt-from-smol-ai-part-1-fa03bcf5209b?source=rss----98111c9905da---4
            <guid ispermalink="false">https://medium.com/p/fa03bcf5209b</guid>
<category><![CDATA[technology]]></category>
<category><![CDATA[management]]></category>
<category><![CDATA[ai]]></category>
<category><![CDATA[mlops]]></category>
<dc:creator><![CDATA[Meir Kanevskiy]]></dc:creator>
<pubdate>Sat, 17 Jun 2023 00:01:51 GMT</pubdate>
<atom:updated>2023-06-17T00:01:51.242Z</atom:updated>
<content:encoded><![CDATA[<h3>Managing an AI developer: Lessons Learned from SMOL AI — Part 1</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*b1B1M32bJatPr-Jv0AET6g.png" /><figcaption>Source: Image by DALL-E</figcaption></figure><p>One of the most interesting ramifications of the recent breakthroughs, specifically in large language models, is the potential for building automated agents capable of fulfilling work projects using these models. The theoretical capabilities here are vast. Sometimes, when communicating with a chat-based model like ChatGPT, a simple follow-up prompt such as “improve your answer” or “make your answer more accurate” can significantly enhance the initial query’s response. However, building such automated agents raises an old-new problem: effective management and responsibility. How do you manage such an agent? Managing real human beings is no simple task and has spawned thousands of pages of literature, as well as popular and polished practices and methodologies based on decades of experience. Can these practices be applied to AI-agent-performed development? What factors should be considered? What metrics should be assigned to projects delivered by AI agents? While we cannot fully answer these questions, let’s consider a specific case and see what we can learn from it.</p><h3>The task</h3><p>In various situations, there is a need to label different entities, such as configurations, datasets, models, color schemes, or any other meaningful group of similar entities in a project. It would be helpful to quickly assign recognizable names to these entities. Recognizable to a human eye (as not in UUID4) and names in plural (as in more than there are recallable colors in a rainbow). Frequent users of e.g. docker cli or wandb might have already recognized the issue. Running docker containers are automatically labeled with rather funny and due to their absurdity easily discernible names like <em>heuristic_einstein</em> or <em>musing_babbage</em>. If we pause right here, a human being reading this article probably needs to read no further to state, execute and deliver the project we’re about to hand over to our AI agent. Our human perception is an amazingly complex thing, utilizing our whole life experience full with different semantics and abstractions we take for granted. We analyze, assume and extrapolate our observations at once, not even thinking of the grand baggage of experience that stands behind those abilities. However, when approaching a language model, one has to be very judicious and humble with regards to one’s understanding of how a model actually thinks. To take on our experiment, we will use the brilliant and light-speed evolving <a href="https://github.com/smol-ai/developer">SMOL-AI’s developer</a>, according to its own readme:</p><blockquote><strong><em>Human-centric &amp; Coherent Whole Program Synthesis</em></strong> aka your own personal junior developer</blockquote><h3>Initial problem statement</h3><p>So, let’s say we want to write a reusable program (not just a snippet) that generates a random name following a naming scheme similar to <a href="https://github.com/moby/moby/blob/4f0d95fa6ee7f865597c03b9e63702cdcb0f7067/pkg/namesgenerator/names-generator.go">that of docker</a>. According to SMOL Dev’s readme, an initial prompt needs to be provided along with the supported model’s API key (in this case, GPT4). The result is evaluated and repeated if necessary.</p><p>So let’s write our initial prompt. Since luckily we don’t have to utilize our own creativity, let’s make the naming scheme a little more nuanced:</p><pre>Please write a naming scheme generator function <br>that generates a random name to the likes of running docker containers,<br>consisting of an adjective and a noun,<br>where adjective is an emotional description, e.g. dramatic,<br>and noun is an abstract entity like manifold.<br>It has to contain up to 7 adjective options for every letter<br>and up to 7 nouns for every letter.</pre><h3>Initial result</h3><p>Having installed necessary python requirements, running smol dev on a prompt is as easy as (using only )</p><pre>export OPENAI_API_KEY=***<br>python main_no_modal.py ./prompts/prompt.md</pre><h4>Scope</h4><p>Smol dev’s working cycle took several minutes and produced the following repo:</p><pre>├── adjectives.py<br>├── naming_scheme_generator.py<br>├── nouns.py<br>└── shared_dependencies.md</pre><p>The main function was quite straightforward and, arguably, the random letter one-liner is even elegant:</p><pre>import random<br>from adjectives import adjectives<br>from nouns import nouns<br><br>def get_random_element(arr):<br>    return random.choice(arr)<br><br>def generate_random_name():<br>    random_letter = chr(97 + random.randint(0, 25))<br>    adjective = get_random_element(adjectives[random_letter])<br>    noun = get_random_element(nouns[random_letter])<br>    return f&quot;{adjective}-{noun}&quot;<br><br>if __name__ == &quot;__main__&quot;:<br>    print(generate_random_name())</pre><p>2 elements are drawing attention here, as they have not been expxlicitly specified by the prompt:</p><ol><li>This file is cmd executable and has a designated __main__ branch.</li><li>Random element choice ids abstracted into its own function</li></ol><h4>Reasoning</h4><p>It’s hard to determine the exact motivation behind introducing these elements into the solution, but if done by a human, the __main__ clause would likely be dictated by the desire to easily turn the standalone function described in the prompt into a ready-to-use CLI tool. This simple improvement would allow the function to be run in the shell and chained with other commands. A suitable term for this could be &quot;MMVP&quot; (Minimal Minimal Value Product), an addon to the prompt resolution that requires minimal additional effort to make it actually usable.</p><p>Abstracting the random choice function, on the other hand, is a more obvious addition that eases expected further modifications.</p><h4>Functionality</h4><p>Correct integration, including importing the corresponding iterable and randomly indexing it with the (almost as we’ll shortly see) correct length constraints, is not surprising to users familiar with GPT’s brilliance. Now let’s examine the components that provide the actual functionality: adjectives.py and nouns.py.</p><p>nouns.py fits our prompt well, with slight deviations from the requested semantic scope requested, which, with all honesty was vague enough to allow that:</p><pre>nouns = {<br>  &#39;a&#39;: [&#39;abyss&#39;, &#39;angel&#39;, &#39;artifact&#39;, &#39;anomaly&#39;, &#39;algorithm&#39;, &#39;atmosphere&#39;, &#39;antenna&#39;],<br>  &#39;b&#39;: [&#39;beacon&#39;, &#39;bubble&#39;, &#39;boundary&#39;, &#39;balance&#39;, &#39;butterfly&#39;, &#39;breeze&#39;, &#39;blossom&#39;],<br>  &#39;c&#39;: [&#39;cosmos&#39;, &#39;catalyst&#39;, &#39;crystal&#39;, &#39;conundrum&#39;, &#39;cipher&#39;, &#39;cascade&#39;, &#39;crescendo&#39;],<br>  &#39;d&#39;: [&#39;dimension&#39;, &#39;dynamo&#39;, &#39;dream&#39;, &#39;duality&#39;, &#39;dawn&#39;, &#39;dusk&#39;, &#39;divergence&#39;],<br>  &#39;e&#39;: [&#39;echo&#39;, &#39;eternity&#39;, &#39;enigma&#39;, &#39;essence&#39;, &#39;evolution&#39;, &#39;eclipse&#39;, &#39;equilibrium&#39;],<br>  &#39;f&#39;: [&#39;frequency&#39;, &#39;flux&#39;, &#39;fountain&#39;, &#39;fusion&#39;, &#39;fable&#39;, &#39;fantasy&#39;, &#39;form&#39;],<br>  &#39;g&#39;: [&#39;galaxy&#39;, &#39;gravity&#39;, &#39;glimmer&#39;, &#39;glow&#39;, &#39;genesis&#39;, &#39;garden&#39;, &#39;groove&#39;],<br>  &#39;h&#39;: [&#39;harmony&#39;, &#39;horizon&#39;, &#39;haze&#39;, &#39;hallucination&#39;, &#39;hologram&#39;, &#39;hypnosis&#39;, &#39;haven&#39;],<br>  &#39;i&#39;: [&#39;infinity&#39;, &#39;illusion&#39;, &#39;insight&#39;, &#39;imagination&#39;, &#39;intuition&#39;, &#39;inception&#39;, &#39;impulse&#39;],<br>  &#39;j&#39;: [&#39;juxtaposition&#39;, &#39;jubilation&#39;, &#39;jigsaw&#39;, &#39;journey&#39;, &#39;jolt&#39;, &#39;junction&#39;, &#39;jazz&#39;],<br>  &#39;k&#39;: [&#39;kaleidoscope&#39;, &#39;karma&#39;, &#39;key&#39;, &#39;kinetic&#39;, &#39;kismet&#39;, &#39;kinesis&#39;, &#39;knight&#39;],<br>  &#39;l&#39;: [&#39;labyrinth&#39;, &#39;light&#39;, &#39;luminescence&#39;, &#39;levitation&#39;, &#39;landscape&#39;, &#39;legacy&#39;, &#39;loop&#39;],<br>  &#39;m&#39;: [&#39;manifold&#39;, &#39;matrix&#39;, &#39;mystery&#39;, &#39;mirage&#39;, &#39;metamorphosis&#39;, &#39;momentum&#39;, &#39;myth&#39;],<br>  &#39;n&#39;: [&#39;nebula&#39;, &#39;nexus&#39;, &#39;nocturne&#39;, &#39;novelty&#39;, &#39;nirvana&#39;, &#39;numen&#39;, &#39;nucleus&#39;],<br>  &#39;o&#39;: [&#39;oasis&#39;, &#39;odyssey&#39;, &#39;oracle&#39;, &#39;oscillation&#39;, &#39;origin&#39;, &#39;overture&#39;, &#39;oblivion&#39;],<br>  &#39;p&#39;: [&#39;paradox&#39;, &#39;portal&#39;, &#39;pulse&#39;, &#39;pattern&#39;, &#39;phantom&#39;, &#39;prophecy&#39;, &#39;paragon&#39;],<br>  &#39;q&#39;: [&#39;quasar&#39;, &#39;quantum&#39;, &#39;quintessence&#39;, &#39;quiver&#39;, &#39;quandary&#39;, &#39;quicksilver&#39;, &#39;quest&#39;],<br>  &#39;r&#39;: [&#39;resonance&#39;, &#39;reverie&#39;, &#39;rhythm&#39;, &#39;radiance&#39;, &#39;reflection&#39;, &#39;rift&#39;, &#39;revelation&#39;],<br>  &#39;s&#39;: [&#39;symphony&#39;, &#39;spectrum&#39;, &#39;shadow&#39;, &#39;silhouette&#39;, &#39;solstice&#39;, &#39;serendipity&#39;, &#39;sanctuary&#39;],<br>  &#39;t&#39;: [&#39;transcendence&#39;, &#39;tesseract&#39;, &#39;twilight&#39;, &#39;tide&#39;, &#39;tempo&#39;, &#39;trance&#39;, &#39;threshold&#39;],<br>  &#39;u&#39;: [&#39;universe&#39;, &#39;utopia&#39;, &#39;unity&#39;, &#39;umbra&#39;, &#39;uplift&#39;, &#39;ultra&#39;, &#39;undulation&#39;],<br>  &#39;v&#39;: [&#39;vortex&#39;, &#39;vision&#39;, &#39;vibration&#39;, &#39;voyage&#39;, &#39;veil&#39;, &#39;vivid&#39;, &#39;vanguard&#39;],<br>  &#39;w&#39;: [&#39;wavelength&#39;, &#39;whisper&#39;, &#39;wonder&#39;, &#39;wisdom&#39;, &#39;web&#39;, &#39;wind&#39;, &#39;wave&#39;],<br>  &#39;x&#39;: [&#39;xenon&#39;, &#39;xanadu&#39;, &#39;x-ray&#39;, &#39;xenolith&#39;, &#39;xylem&#39;, &#39;xenogenesis&#39;, &#39;xylophone&#39;],<br>  &#39;y&#39;: [&#39;yonder&#39;, &#39;yearning&#39;, &#39;yugen&#39;, &#39;yin&#39;, &#39;yang&#39;, &#39;yawn&#39;, &#39;yesteryear&#39;],<br>  &#39;z&#39;: [&#39;zenith&#39;, &#39;zephyr&#39;, &#39;zone&#39;, &#39;zodiac&#39;, &#39;zigzag&#39;, &#39;zombie&#39;, &#39;zeppelin&#39;]<br>}</pre><p>adjectives.py, however, reveals an interesting surprise:</p><pre>adjectives = {<br>    &#39;a&#39;: [&#39;angry&#39;, &#39;anxious&#39;, &#39;amused&#39;, &#39;adorable&#39;, &#39;amazing&#39;, &#39;ambitious&#39;, &#39;artistic&#39;],<br>    &#39;b&#39;: [&#39;brave&#39;, &#39;bored&#39;, &#39;bitter&#39;, &#39;bold&#39;, &#39;bashful&#39;, &#39;bewildered&#39;, &#39;benevolent&#39;],<br>    # ...<br>    &#39;z&#39;: [&#39;zealous&#39;, &#39;zany&#39;, &#39;zestful&#39;, &#39;zen&#39;, &#39;zippy&#39;, &#39;zombie-like&#39;, &#39;zigzag&#39;]<br>}</pre><p>This dictionary would obviously be unusable in our case and would cause an unhandled exception if a letter other than ‘a’, ‘b’, or ‘z’ is chosen.</p><h4>Reasoning</h4><p>When done by a human, such a mistake would be a classic evidence of lack of testing:</p><ol><li>A functional block was implemented with an incomplete placeholder.</li><li>Sufficient testing was not performed.</li><li>The placeholder was left in the final solution.</li></ol><p>The interesting deviation in our case is the involvement of two actors: the SMOL AI agent and the actual content supplier in the form of a language model. As is evident from the length of the supplied lists, this dictionary was intended to be a finalized answer to the query for supplying the adjectives component. However, the language model sometimes omits repetitive code and provides an initial example without completing it. In a chat use-case, this often takes the form of an answer containing general directions for performing a task instead of actual code. This occurs when the prompt does not emphasize providing code specifically and is more of a general question on a matter.</p><p>Even less “human” in this case is the mismatch between the attention to detail in the main file we described above and this incompleteness being overlook.</p><h3>Improvement</h3><h4>Requirements</h4><p>Taking above considerations into account, let’s engineer a more advanced prompt. This time, we’ll describe our needs in a formal manner, more suitable to how an actual project might be described, while adding some useful degrees of freedom:</p><pre>This project is a naming scheme generator.<br>It has to be subject to the following tech stack:<br>1. Programming language: wherever possible, but not limited to python<br>It has to be subject to the following specification:<br>1. Having an importable and callable method in python that returns a random name<br>2. Said name has to be a pair of an adjective and a noun starting with the same letter to the likes of docker container naming convention<br>3. Adjective and noun have to be of specific theme. Default themes should emotional description, e.g. dramatic for adjective and an abstract entity like manifold for noun.<br>4. Apart from the default themes, theme pair should be customizable by providing a json with list of available options per each letter of the English alphabet. Providing the json can be constrained to a certain folder inside the project structure.<br>5. it has to have tests, evaluating extensiveness of fulfilling the above specifications</pre><ol><li>Stating the tech stack: It is by chance that the provided solution was in Python. We never explicitly asked for it, and the referenced Docker naming scheme is, expectedly, written in Go.</li><li>Specifying the use-case: We wouldn’t want to leave the usability of our solution to the model’s consideration alone. As we’ve seen, it can be reduced to an MMVP, as described above.</li><li>Static data parametrization: Although the referenced Docker naming scheme is hardcoded, generally one might want to avoid such an approach, especially for the central functionality of the project.</li><li>Testing: as we’ve seen, as opposed to adding cli usability of its own accord, the model did not provide any unit tests for the solution. This had serious ramifications, resulting in a completely broken key component (the adjectives list).</li></ol><h4>Improved result</h4><p>This time the run took 11 minutes and produced a much more elaborate repo structure:</p><pre>├── naming_scheme_generator<br>│   ├── generator.py<br>│   ├── __init__.py<br>│   └── themes<br>│       ├── custom_theme_example.json<br>│       ├── default_theme.json<br>│       └── __init__.py<br>├── shared_dependencies.md<br>└── tests<br>    ├── __init__.py<br>    ├── test_generator.py<br>    └── test_themes.py</pre><p>This structure is sound and well-aligned with our above requirements. The word registry has been moved to a single JSON file, correctly listing all the letters combined in the following format. You can notice that the model returned a minimal plural number of 2, as we did not provide a prerequisite for the number of words per letter:</p><pre>&quot;A&quot;: {<br>    &quot;adjectives&quot;: [&quot;anxious&quot;, &quot;amazing&quot;],<br>    &quot;nouns&quot;: [&quot;artifact&quot;, &quot;ambience&quot;]<br>  }</pre><p>generator.py showed another interesting change. Note that the improvised &quot;MMVP&quot; in the form of a __main__ branch for CLI usage is now absent, as we explicitly stated that the usage should be an <em>&quot;importable and callable method in Python&quot;</em>:</p><pre>import random<br>from typing import Dict, List<br>from .themes import default_theme<br><br>def generate_name(theme_adjective: Dict[str, List[str]], theme_noun: Dict[str, List[str]]) -&gt; str:<br>    letter = random.choice(list(theme_adjective.keys()))<br>    adjective = random.choice(theme_adjective[letter])<br>    noun = random.choice(theme_noun[letter])<br>    return f&quot;{adjective}_{noun}&quot;<br><br>def load_custom_theme(file_path: str) -&gt; Dict[str, List[str]]:<br>    with open(file_path, &quot;r&quot;) as file:<br>        custom_theme = json.load(file)<br>    return custom_theme<br><br>def generate_name_with_theme(theme: str = &quot;default&quot;) -&gt; str:<br>    if theme == &quot;default&quot;:<br>        theme_adjective = default_theme[&quot;adjectives&quot;]<br>        theme_noun = default_theme[&quot;nouns&quot;]<br>    else:<br>        custom_theme = load_custom_theme(theme)<br>        theme_adjective = custom_theme[&quot;adjectives&quot;]<br>        theme_noun = custom_theme[&quot;nouns&quot;]<br><br>    return generate_name(theme_adjective, theme_noun)</pre><p>Multiple problems become apparent here:</p><ul><li>Import errors: The json library import is missing.</li><li>The .themes folder (leaving aside the taste question of using . imports) does not have any Python-importable default_theme. Instead, it&#39;s a JSON file meant to be read from Python code. The same error is present in the __init__.py file of the themes folder:</li></ul><pre>from .default_theme import default_theme<br>from .custom_theme_example import custom_theme_example</pre><ul><li>The theme dictionary, if read as-is from the JSON file, has a structure opposite to that implied by the code. The theme itself doesn’t have “adjective” and “noun” keys. Its keys are letters of the alphabet, each nested with adjectives and nouns.</li></ul><p>Due to the above reasons, the written tests are failing on the imports stage and cannot be executed. However, their usage of imported functions is correct, and their extensiveness is genuinely impressive:</p><pre>import unittest<br>from naming_scheme_generator.generator import generate_name, load_custom_theme<br>from naming_scheme_generator.themes import default_theme<br><br>class TestGenerator(unittest.TestCase):<br><br>    def test_generate_name_default_theme(self):<br>        name = generate_name(default_theme.adjectives, default_theme.nouns)<br>        self.assertIsNotNone(name)<br>        self.assertTrue(isinstance(name, str))<br>        self.assertEqual(len(name.split(&quot; &quot;)), 2)<br>        self.assertEqual(name.split(&quot; &quot;)[0][0], name.split(&quot; &quot;)[1][0])<br><br>    def test_generate_name_custom_theme(self):<br>        custom_theme = load_custom_theme(&quot;naming_scheme_generator/themes/custom_theme_example.json&quot;)<br>        name = generate_name(custom_theme.adjectives, custom_theme.nouns)<br>        self.assertIsNotNone(name)<br>        self.assertTrue(isinstance(name, str))<br>        self.assertEqual(len(name.split(&quot; &quot;)), 2)<br>        self.assertEqual(name.split(&quot; &quot;)[0][0], name.split(&quot; &quot;)[1][0])<br><br>    def test_load_custom_theme(self):<br>        custom_theme = load_custom_theme(&quot;naming_scheme_generator/themes/custom_theme_example.json&quot;)<br>        self.assertIsNotNone(custom_theme)<br>        self.assertTrue(hasattr(custom_theme, &quot;adjectives&quot;))<br>        self.assertTrue(hasattr(custom_theme, &quot;nouns&quot;))<br><br>if __name__ == &#39;__main__&#39;:<br>    unittest.main()</pre><h4>Intermediate conclusions</h4><p>With the above said and considering the very limited scope of our “test project,” which hardly allows for a component breakdown, one could argue that:</p><ul><li>In the initial case, the “core functionality” of the solution was faulty. Even though the utility code was correct, the human supervisor would have had to come up with the missing words, which were plenty. Additional effort in this case would be of a creative nature.</li><li>In the latter case, the necessary improvements are technical. Missing imports need to be corrected, the default theme workflow needs to be changed from a Pythonic import to a JSON read (similar to the already correct custom theme), and the nesting levels in the word dictionary need to be swapped.</li><li>In the latter case, the project infrastructure is correct. Theme customization is separated from the main generator code, and extensive tests are correctly organized and separated into a distinct folder.</li></ul><p>The fallbacks of the second solution are much easier to correct and can be delegated to a junior developer role, while the fallbacks of the first case would correspond to a major core algorithm issue in a more complex project, requiring the involvement of a more senior and qualified human supervisor.</p><p>Of course, it’s also worth mentioning that with all of the above, the achievements that SMOL AI contributors have been able to attain in such a short time are fascinating. The magnificence of the latest developments in large language models makes it easy to theorize about their automation. However, bringing a solution like this to an actual usable implementation is a different class of achievement.</p><p>In Part 2 we’ll take a look at further iterations and see if SMOL Dev can actually become that junior developer and improve its own results.</p><p>Thank you for reading!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fa03bcf5209b" width="1" height="1" alt=""><hr><p><a href="https://pub.towardsai.net/managing-an-ai-developer-lessons-learnt-from-smol-ai-part-1-fa03bcf5209b">Managing an AI developer: lessons learnt from SMOL AI — Part 1</a> was originally published in <a href="https://pub.towardsai.net">Towards AI</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
</item>
<item>
<title><![CDATA[The future of Artificial Intelligence is Open-source! Here’s why?]]></title>
<description><![CDATA[<div class="medium-feed-item"><p class="medium-feed-image"><a href="https://pub.towardsai.net/the-future-of-artificial-intelligence-is-open-source-heres-why-af758b30f1a7?source=rss----98111c9905da---4"><img src="https://cdn-images-1.medium.com/max/2600/0*KxrQlt2GAAgdGVkq" width="3936"></a></p><p class="medium-feed-snippet">Open-source AI gets Big Tech in trouble</p><p class="medium-feed-link"><a href="https://pub.towardsai.net/the-future-of-artificial-intelligence-is-open-source-heres-why-af758b30f1a7?source=rss----98111c9905da---4">Continue reading on Towards AI »</a></p></div>]]></description>
<link/>https://pub.towardsai.net/the-future-of-artificial-intelligence-is-open-source-heres-why-af758b30f1a7?source=rss----98111c9905da---4
            <guid ispermalink="false">https://medium.com/p/af758b30f1a7</guid>
<category><![CDATA[artificial-intelligence]]></category>
<category><![CDATA[data-science]]></category>
<category><![CDATA[large-language-models]]></category>
<category><![CDATA[entrepreneurship]]></category>
<category><![CDATA[open-source]]></category>
<dc:creator><![CDATA[Janik and Patrick Tinz]]></dc:creator>
<pubdate>Fri, 16 Jun 2023 22:01:28 GMT</pubdate>
<atom:updated>2023-06-16T22:01:28.352Z</atom:updated>
</item>
<item>
<title><![CDATA[Decoding the Binomial Distribution: A Fundamental Concept for Data Scientists]]></title>
<description><![CDATA[<div class="medium-feed-item"><p class="medium-feed-image"><a href="https://pub.towardsai.net/decoding-the-binomial-distribution-a-fundamental-concept-for-data-scientists-81c64c7e4580?source=rss----98111c9905da---4"><img src="https://cdn-images-1.medium.com/max/2600/0*C7HRUKJcvbwx6iNV" width="4178"></a></p><p class="medium-feed-snippet">Understanding the basic building blocks of the binomial distribution</p><p class="medium-feed-link"><a href="https://pub.towardsai.net/decoding-the-binomial-distribution-a-fundamental-concept-for-data-scientists-81c64c7e4580?source=rss----98111c9905da---4">Continue reading on Towards AI »</a></p></div>]]></description>
<link/>https://pub.towardsai.net/decoding-the-binomial-distribution-a-fundamental-concept-for-data-scientists-81c64c7e4580?source=rss----98111c9905da---4
            <guid ispermalink="false">https://medium.com/p/81c64c7e4580</guid>
<category><![CDATA[optimization]]></category>
<category><![CDATA[machine-learning]]></category>
<category><![CDATA[data-science]]></category>
<category><![CDATA[statistics]]></category>
<category><![CDATA[artificial-intelligence]]></category>
<dc:creator><![CDATA[Egor Howell]]></dc:creator>
<pubdate>Fri, 16 Jun 2023 20:01:48 GMT</pubdate>
<atom:updated>2023-06-16T20:01:48.187Z</atom:updated>
</item>
<item>
<title><![CDATA[The What’s AI Podcast Episode 15: Luis Serrano on AI Education and LLMs]]></title>
<description><![CDATA[<div class="medium-feed-item"><p class="medium-feed-image"><a href="https://pub.towardsai.net/the-whats-ai-podcast-episode-15-luis-serrano-on-ai-education-and-llms-ed38d98ef2fa?source=rss----98111c9905da---4"><img src="https://cdn-images-1.medium.com/max/1280/1*b8PQuqfp6uBzD6ywOV7knA.png" width="1280"></a></p><p class="medium-feed-snippet">Why learn more about LLMs and improve your communication skills</p><p class="medium-feed-link"><a href="https://pub.towardsai.net/the-whats-ai-podcast-episode-15-luis-serrano-on-ai-education-and-llms-ed38d98ef2fa?source=rss----98111c9905da---4">Continue reading on Towards AI »</a></p></div>]]></description>
<link/>https://pub.towardsai.net/the-whats-ai-podcast-episode-15-luis-serrano-on-ai-education-and-llms-ed38d98ef2fa?source=rss----98111c9905da---4
            <guid ispermalink="false">https://medium.com/p/ed38d98ef2fa</guid>
<category><![CDATA[llm]]></category>
<category><![CDATA[ai]]></category>
<category><![CDATA[openai]]></category>
<category><![CDATA[nlp]]></category>
<dc:creator><![CDATA[Louis Bouchard]]></dc:creator>
<pubdate>Fri, 16 Jun 2023 18:02:09 GMT</pubdate>
<atom:updated>2023-06-16T18:02:09.515Z</atom:updated>
</item>
<item>
<title><![CDATA[19 Most Elegant Sklearn Tricks I Found After 3 Years of Use]]></title>
<description><![CDATA[<div class="medium-feed-item"><p class="medium-feed-image"><a href="https://pub.towardsai.net/19-most-elegant-sklearn-tricks-i-found-after-3-years-of-use-5bda439fa506?source=rss----98111c9905da---4"><img src="https://cdn-images-1.medium.com/max/1456/1*bv2KUVNLi2sFNjBTdoBmWw.png" width="1456"></a></p><p class="medium-feed-snippet">Advanced techniques and hidden gems for effective machine learning</p><p class="medium-feed-link"><a href="https://pub.towardsai.net/19-most-elegant-sklearn-tricks-i-found-after-3-years-of-use-5bda439fa506?source=rss----98111c9905da---4">Continue reading on Towards AI »</a></p></div>]]></description>
<link/>https://pub.towardsai.net/19-most-elegant-sklearn-tricks-i-found-after-3-years-of-use-5bda439fa506?source=rss----98111c9905da---4
            <guid ispermalink="false">https://medium.com/p/5bda439fa506</guid>
<category><![CDATA[data-science]]></category>
<category><![CDATA[machine-learning]]></category>
<category><![CDATA[programming]]></category>
<category><![CDATA[artificial-intelligence]]></category>
<category><![CDATA[python]]></category>
<dc:creator><![CDATA[Bex T.]]></dc:creator>
<pubdate>Fri, 16 Jun 2023 16:02:08 GMT</pubdate>
<atom:updated>2023-06-16T16:02:07.282Z</atom:updated>
</item>
<item>
<title><![CDATA[Bagging vs. Boosting: The Power of Ensemble Methods in Machine Learning]]></title>
<description><![CDATA[<div class="medium-feed-item"><p class="medium-feed-image"><a href="https://pub.towardsai.net/bagging-vs-boosting-the-power-of-ensemble-methods-in-machine-learning-6404e33524e6?source=rss----98111c9905da---4"><img src="https://cdn-images-1.medium.com/max/1153/1*XzaoQxMf4uLD5DIHz8JwbA.png" width="1153"></a></p><p class="medium-feed-snippet">How to maximize predictive performance by creating a strong learner from multiple weak ones</p><p class="medium-feed-link"><a href="https://pub.towardsai.net/bagging-vs-boosting-the-power-of-ensemble-methods-in-machine-learning-6404e33524e6?source=rss----98111c9905da---4">Continue reading on Towards AI »</a></p></div>]]></description>
<link/>https://pub.towardsai.net/bagging-vs-boosting-the-power-of-ensemble-methods-in-machine-learning-6404e33524e6?source=rss----98111c9905da---4
            <guid ispermalink="false">https://medium.com/p/6404e33524e6</guid>
<category><![CDATA[artificial-intelligence]]></category>
<category><![CDATA[data-science]]></category>
<category><![CDATA[machine-learning]]></category>
<category><![CDATA[science]]></category>
<category><![CDATA[technology]]></category>
<dc:creator><![CDATA[Thomas A Dorfer]]></dc:creator>
<pubdate>Fri, 16 Jun 2023 14:03:46 GMT</pubdate>
<atom:updated>2023-06-16T14:03:46.125Z</atom:updated>
</item>
<item>
<title><![CDATA[Summertime Sadness ft. PandasAI]]></title>
<link/>https://pub.towardsai.net/summertime-sadness-ft-pandasai-ce47aec70d90?source=rss----98111c9905da---4
            <guid ispermalink="false">https://medium.com/p/ce47aec70d90</guid>
<category><![CDATA[optimisation]]></category>
<category><![CDATA[pandas]]></category>
<category><![CDATA[large-language-models]]></category>
<category><![CDATA[llm]]></category>
<category><![CDATA[machine-learning]]></category>
<dc:creator><![CDATA[Soumyadip Mal]]></dc:creator>
<pubdate>Fri, 16 Jun 2023 14:02:43 GMT</pubdate>
<atom:updated>2023-06-16T14:02:43.309Z</atom:updated>
<content:encoded><![CDATA[<h3>Summertime Sadness ft. PandasAI: Deep-Dive</h3><p>PandasAI — a potential game-changer for Data Engineers or just a gimmick? Let’s find out!</p><p>Spent the entire day with this nifty little library, dismantled it bit by bit, and, here are my 2 cents on it.</p><p>I’m not particularly good at prose-so I’ll just approach my first “tech blog”, the way I know - without any frills. Also, it’s well past 3 am at the time of writing this, so you know. First things first:</p><p><strong>1) What Kind of LLMs fuelling it? </strong>OpenAI-based models, Huggingface LLM(falcon), Starcoder and the PALM suit of models from Google. Have solely tried with OpenAI for now. For OpenAI, the defaults are:</p><blockquote><strong>‘temperature’: 0,<br> ‘max_tokens’: 512,<br> ‘top_p’: 1,<br> ‘frequency_penalty’: 0,<br> ‘presence_penalty’: 0.6,<br> ‘model’: ‘gpt-3.5-turbo’</strong></blockquote><p><strong>2)Is it free?</strong> The base usage remains free of course, but the conversational flavor with OpenAI LLM would require an API key and hence comes at a cost, the cost of GPT3.5, to be precise.</p><p><strong>3)Under the hood</strong> — At a very abstract level, it can either work with single dataframes or a single list of DataFrames. Taking the example of a single data frame, it takes into consideration only the head with 5 rows. Internally forms a hardcoded prompt and sends it through the chat completion endpoint to get the Python code representation of the input query/question. And then, it runs the Python code on the entire of the original dataframe to get the desired result. The underlying assumption here being-you don&#39;t really need to see the entire dataset to form a general query; the column names are quite enough. Also, this saves on the token usage front. As simple as that.</p><p><strong>4)Data Privacy?</strong> There’s an option to init call with “<strong><em>enforce_privacy</em></strong>” flag set to False, so that the df.head() doesn’t show up in the prompt. And since it doesn’t upload your entire table anyway, just the headers only, there shouldn’t be any concern related to data privacy.</p><p><strong>5)Is the output conversational? </strong>Yes, there’s another init argument called “conversational” that defaults to false but can be set to True to give a more conversational type of answer.</p><p><strong>6) Middleware?</strong> Those coming from Django would be aware of this, something that we used to do with monkey-patching in yesteryears. PandasAI also supports sending in a list of middleware classes as input param.</p><h3><strong>Diving In:</strong></h3><h4>Working with a single dataframe —</h4><p>With <em>conversational</em>=<em>True(set the token in line 15 as your API key)</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ilMCr3aX4W7gbBXRmqdclQ.png" /></figure><p>With <em>conversational</em>=<em>False(set the token in line 15 as your API key)</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Z_64Q8GUwyyTAS_wPtST9A.png" /></figure><p>Prompts formed under the hood for the default non-conversational flavor with single df, with <em>enforce_privacy</em> set to <em>False</em>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-E_zYjsXF6x7mQDqaO8glA.png" /></figure><p>Prompts formed under the hood for the default non-conversational flavor with single df, with <em>enforce_privacy</em> set to <em>True</em>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5Z_sLJp-oGaB5c66cLcn2A.png" /></figure><p>As visible, only the col. names are there in the prompt with the privacy turned on, actual data is discarded.</p><h4>Working with a list of dataframes —</h4><p>Notice the additional data frame in line 14, which is being passed in a list along with the original df in line 28. Have intentionally removed the happiness index col from the new df and modified the GDP of the United States. Also, the query now says “USA” instead of “United States”. Here in lies the magic of LLMs, it was able to still form the query.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ayrj1iPxiJlgo73ZwLecsQ.png" /></figure><p>Prompts formed with <em>enforce_privacy</em> set to <em>True</em>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RXMml9HcGlt4uIpGSuQGvQ.png" /></figure><p>Prompts formed with <em>enforce_privacy</em> set to <em>False</em>:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3TcKtmeKXZfRsTJ6GtZKiA.png" /></figure><p>The privacy thing failed for multiple df’s due to a small bug in the code (I have sent a pull request, let’s see).</p><h3>Optimisation Proposed:</h3><p>Leaving aside the multiple df case for now. For single df, with privacy enabled, prompts are a little shorter(owing to the lack of example data from the head) and thus less costly, but for the sake of better prompting practice, let’s keep it set to False for now.</p><blockquote>pandas_ai = PandasAI(llm,verbose=True,conversational=False,enforce_privacy=False,enable_cache=False)</blockquote><blockquote>Input df:</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/512/1*AqO6RiJCd8OmOxaN25a_pQ.png" /></figure><blockquote>Input query: Which is the happiest country?</blockquote><blockquote>Input request body: (Clearly, has way too many new line characters and other special chars, that contribute to the token-count and essentially, the cost incurred for each api call)</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-QrRJa__kQNM1fv_HR_Crw.png" /></figure><blockquote>Response</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*BVccD9LiZ4NaRd39-Eauzg.png" /></figure><p>So, 376 total tokens for a df with 10 rows and 3 cols. GPT3.5-Turbo is priced at $0.002 / 1K tokens. <strong>So, for 10,000 such hits, we’d get around 3760k tokens and it could cost us 7.52 USD.</strong></p><p>However, if we slightly tweak the input prompt to hold the df in the form of a comma-separated format(csv) instead of a string representation of the head(5), here’s what happens:</p><blockquote>Input request body:(Notice the comma separated fields)</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*q6_KaUpp3yLm530tr5_ZDQ.png" /></figure><p>So much more compact.</p><blockquote>Response:</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5fqfUiihr0uI_fwQbc5xxQ.png" /></figure><h3><strong>So, a total of 340 tokens. That’s a 9.5% cost reduction overall!</strong></h3><p>For now, have just made the change for the single data-frame input. Will put in a PR for the multi-df list too.</p><p>Well, that was it, folks. Will try to spend some time with Scikit-LLM next. I quite liked the idea here, just send the df skeleton to LLM and get the pandas equivalent code, and then run it on the actual dataset for the result. Text to SQL and vice-versa have been there for a while; this just takes it up a notch — Text to Pandas. This also opens up a lot of other avenues — SQL query optimization, performance tuning of code, etc. Now, pandas query optimization could very well be a thing too! Other Python libs, I imagine, would follow this trend too.</p><p>— . — — — — -.. -… -. — . ( Morse for something, idk)</p><h3><strong>Thanks for reading!</strong></h3><p><em>Your feedback and questions are highly appreciated. You can find me on </em><a href="https://www.linkedin.com/in/soumya1729/"><em>LinkedIn</em></a><em> and </em><a href="https://www.instagram.com/soumya1729/"><em>Instagram</em></a><em> or connect with me via Twitter @</em><a href="https://twitter.com/soumyadip_mal"><em>soumyadip_mal</em></a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ce47aec70d90" width="1" height="1" alt=""><hr><p><a href="https://pub.towardsai.net/summertime-sadness-ft-pandasai-ce47aec70d90">Summertime Sadness ft. PandasAI</a> was originally published in <a href="https://pub.towardsai.net">Towards AI</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
</item>
<item>
<title><![CDATA[How AI is Used to Combat Social Engineering Attacks — Part 1]]></title>
<description><![CDATA[<div class="medium-feed-item"><p class="medium-feed-image"><a href="https://pub.towardsai.net/how-ai-is-used-to-combat-social-engineering-attacks-part-1-faf38abde78a?source=rss----98111c9905da---4"><img src="https://cdn-images-1.medium.com/max/1024/1*-hFwjoe4BacX1baw-qsruQ.png" width="1024"></a></p><p class="medium-feed-snippet">An Analysis of AI Approaches to Counteract URL Phishing</p><p class="medium-feed-link"><a href="https://pub.towardsai.net/how-ai-is-used-to-combat-social-engineering-attacks-part-1-faf38abde78a?source=rss----98111c9905da---4">Continue reading on Towards AI »</a></p></div>]]></description>
<link/>https://pub.towardsai.net/how-ai-is-used-to-combat-social-engineering-attacks-part-1-faf38abde78a?source=rss----98111c9905da---4
            <guid ispermalink="false">https://medium.com/p/faf38abde78a</guid>
<category><![CDATA[artificial-intelligence]]></category>
<category><![CDATA[data-science]]></category>
<category><![CDATA[ai]]></category>
<category><![CDATA[cybersecurity]]></category>
<category><![CDATA[machine-learning]]></category>
<dc:creator><![CDATA[John Adeojo]]></dc:creator>
<pubdate>Fri, 16 Jun 2023 13:59:58 GMT</pubdate>
<atom:updated>2023-06-16T13:59:58.191Z</atom:updated>
</item>
<item>
<title><![CDATA[How AI is Used to Combat Social Engineering Attacks — Part 2]]></title>
<description><![CDATA[<div class="medium-feed-item"><p class="medium-feed-image"><a href="https://pub.towardsai.net/how-ai-is-used-to-combat-social-engineering-attacks-part-2-6967f12d8ead?source=rss----98111c9905da---4"><img src="https://cdn-images-1.medium.com/max/1024/1*SgZXn7JlLuiNY8Mmrl9ekg.png" width="1024"></a></p><p class="medium-feed-snippet">An Analysis of AI Approaches to Counteract URL Phishing</p><p class="medium-feed-link"><a href="https://pub.towardsai.net/how-ai-is-used-to-combat-social-engineering-attacks-part-2-6967f12d8ead?source=rss----98111c9905da---4">Continue reading on Towards AI »</a></p></div>]]></description>
<link/>https://pub.towardsai.net/how-ai-is-used-to-combat-social-engineering-attacks-part-2-6967f12d8ead?source=rss----98111c9905da---4
            <guid ispermalink="false">https://medium.com/p/6967f12d8ead</guid>
<category><![CDATA[cybersecurity]]></category>
<category><![CDATA[data-science]]></category>
<category><![CDATA[machine-learning]]></category>
<category><![CDATA[artificial-intelligence]]></category>
<category><![CDATA[ai]]></category>
<dc:creator><![CDATA[John Adeojo]]></dc:creator>
<pubdate>Fri, 16 Jun 2023 13:57:37 GMT</pubdate>
<atom:updated>2023-06-16T13:57:37.581Z</atom:updated>
</item>
<item>
<title><![CDATA[OpenAI Just Introduced Function Callings Feature: Everything You Need to Know]]></title>
<description><![CDATA[<div class="medium-feed-item"><p class="medium-feed-image"><a href="https://pub.towardsai.net/openai-just-introduced-function-callings-feature-everything-you-need-to-know-f42c42ac38df?source=rss----98111c9905da---4"><img src="https://cdn-images-1.medium.com/max/2600/0*j5_gyHnG6aNab-GH" width="5724"></a></p><p class="medium-feed-snippet">The Most Powerful Update to Their API Since Its Release</p><p class="medium-feed-link"><a href="https://pub.towardsai.net/openai-just-introduced-function-callings-feature-everything-you-need-to-know-f42c42ac38df?source=rss----98111c9905da---4">Continue reading on Towards AI »</a></p></div>]]></description>
<link/>https://pub.towardsai.net/openai-just-introduced-function-callings-feature-everything-you-need-to-know-f42c42ac38df?source=rss----98111c9905da---4
            <guid ispermalink="false">https://medium.com/p/f42c42ac38df</guid>
<category><![CDATA[ai]]></category>
<category><![CDATA[openai-api]]></category>
<category><![CDATA[openai]]></category>
<category><![CDATA[chatgpt]]></category>
<category><![CDATA[api]]></category>
<dc:creator><![CDATA[Youssef Hosni]]></dc:creator>
<pubdate>Fri, 16 Jun 2023 00:01:52 GMT</pubdate>
<atom:updated>2023-06-16T09:10:20.533Z</atom:updated>
</item>
</channel>
</rss>